# -*- coding: utf-8 -*-
"""
Monitor de promo√ß√µes de transfer√™ncia de milhas e hotelaria (inclui Accor/ALL).
- Captura blogs/feeds de viagens e p√°ginas HTML selecionadas.
- Aceita posts sobre Smiles vindos de blogs, mas EXCLUI links de dom√≠nio smiles.com.br.
Requisitos: feedparser, beautifulsoup4, html5lib, requests, python-dateutil
Credenciais: arquivo 'credenciais.txt' com 3 linhas (email, senha_app, destino)
"""
import csv, os, re, smtplib, sys
from datetime import datetime, timedelta, timezone
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from urllib.parse import urlparse, urljoin

import feedparser, requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

RECENCIA_DIAS = int(os.getenv("RECENCIA_DIAS", "3"))

# --- Padr√µes de filtro ---
POSITIVE_TERMS = [
    # gen√©ricos
    r"\b(b√¥nus|bonus)\b",
    r"\b(bonifica(√ß|c)√£o|bonificado)\b",
    r"\btransfer(√™|e)ncia(s)?\b",
    r"\btransferir\b",
    r"\bpromo(√ß√£o|cao|s)\b",
    r"\boferta(s)?\b",
    # bancos/ programas a√©reos
    r"\blatam pass\b", r"\btudo ?azul\b", r"\besfera\b", r"\blivelo\b",
    r"\bsantander\b", r"\bbradesco\b", r"\bit(au|a)u\b", r"\bbanco do brasil\b",
    r"\bc6 bank\b", r"\binter\b",
    # smiles (permitido por termo, mas dom√≠nio smiles √© bloqueado abaixo)
    r"\bsmiles\b",
    # hotelaria ‚Äì Accor/ALL
    r"\baccor\b",
    r"\blive limitless\b",
    r"\ball ?- ?accor\b",
    r"\ball ?accor\b",
    r"\b(all|accor)\s+signature\b",
]
NEGATIVE_TERMS = []

# Bloqueia o site oficial da Smiles, mas N√ÉO bloqueia blogs que falem de Smiles
EXCLUDED_DOMAINS = {
    "smiles.com.br", "blog.smiles.com.br", "loja.smiles.com.br"
}

# --- Fontes ---
# Feeds RSS de blogs / portais. (Adicione/retire livremente).
RSS_SOURCES = {
    # j√° usados
    "Passageiro de Primeira": "https://passageirodeprimeira.com/feed/",
    "Melhores Destinos": "https://www.melhoresdestinos.com.br/feed",
    "Pontos pra Viajar": "https://pontospraviajar.com/feed/",
    "Pontos para Voar": "https://www.pontospravoar.com/feed/",
    "Meu Milh√£o de Milhas": "https://meumilhaodemilhas.com/feed/",
    "Passagens Imperd√≠veis": "https://www.passagensimperdiveis.com.br/feed/",
    # novos
    "Viaje na Viagem": "https://www.viajenaviagem.com/feed/",
    "Tudo Viagem": "https://www.tudoviagem.com/feed/",
    "Passagens Promo (blog)": "https://blog.passagenspromo.com.br/feed/",
    "Falando de Viagem (blog)": "https://www.falandodeviagem.com.br/feed.php",  # pode n√£o ter todos os t√≥picos
    "Infoviajantes": "https://infoviajantes.com.br/feed/",
}

# P√°ginas HTML para varredura simples (sem RSS) ‚Äì √∫til para promo√ß√µes oficiais.
HTML_SOURCES = {
    # LATAM Pass oficial
    "LATAM Pass - Promo√ß√µes": "https://latampass.latam.com/pt_br/promocoes",
    # Accor / ALL ‚Äì algumas rotas comuns (se alguma quebrar, o script apenas ignora)
    "Accor ALL - Promo√ß√µes (EN)": "https://all.accor.com/loyalty-program/promotions.en.shtml",
    "Accor ALL - Ofertas Brasil": "https://all.accor.com/brasil/promocoes.shtml",
}

# --- Utilit√°rios ---
def limpar_html(txt):
    if not txt:
        return ""
    try:
        return BeautifulSoup(txt, "html5lib").get_text(" ", strip=True)
    except Exception:
        return re.sub("<[^>]+>", " ", txt)

def dentro_recencia(dt, dias=RECENCIA_DIAS):
    if not dt:
        return False
    return (datetime.now(timezone.utc) - dt) <= timedelta(days=dias)

def parse_datetime(entry):
    for c in [getattr(entry, "published", None), getattr(entry, "updated", None),
              entry.get("published"), entry.get("updated")]:
        if not c:
            continue
        try:
            d = dtparser.parse(c)
            if not d.tzinfo:
                d = d.replace(tzinfo=timezone.utc)
            return d.astimezone(timezone.utc)
        except Exception:
            pass
    return None

def tem_match(padroes, texto):
    for p in padroes:
        if re.search(p, texto, flags=re.IGNORECASE):
            return True
    return False

def dominio_excluido(link):
    try:
        host = urlparse(link).netloc.lower()
        return any(host == d or host.endswith("." + d) for d in EXCLUDED_DOMAINS)
    except Exception:
        return False

# --- Coletas ---
def coletar_rss():
    itens = []
    for nome, url in RSS_SOURCES.items():
        try:
            feed = feedparser.parse(url)
            for e in feed.entries:
                title = limpar_html(getattr(e, "title", "") or e.get("title", ""))
                summary = limpar_html(getattr(e, "summary", "") or e.get("summary", ""))
                link = getattr(e, "link", "") or e.get("link", "")
                dt_pub = parse_datetime(e)
                if not title or not link:
                    continue
                if dominio_excluido(link):
                    continue
                if not dentro_recencia(dt_pub):
                    continue
                texto = f"{title} {summary}".lower()
                if tem_match(NEGATIVE_TERMS, texto):
                    continue
                if not tem_match(POSITIVE_TERMS, texto):
                    continue
                itens.append({
                    "fonte": nome,
                    "titulo": title.strip(),
                    "resumo": summary.strip(),
                    "link": link.strip(),
                    "publicado_em": dt_pub.isoformat() if dt_pub else "",
                    "metodo": "RSS",
                })
        except Exception as ex:
            print(f"[WARN] RSS falhou {nome}: {ex}")
    return itens

def http_get(url, timeout=20):
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0 PromoBot"}, timeout=timeout)
        if r.status_code == 200 and r.text:
            return r
    except Exception as ex:
        print(f"[WARN] GET falhou {url}: {ex}")
    return None

def coletar_html():
    itens = []
    for nome, url in HTML_SOURCES.items():
        resp = http_get(url)
        if not resp:
            continue
        try:
            soup = BeautifulSoup(resp.text, "html5lib")
            for a in soup.select("a[href]"):
                href = a.get("href") or ""
                texto = a.get_text(" ", strip=True) or ""
                if not href or href.startswith("#"):
                    continue
                if href.startswith("/"):
                    href = urljoin(url, href)
                if dominio_excluido(href):
                    continue
                alvo = f"{texto} {href}".lower()
                if tem_match(NEGATIVE_TERMS, alvo):
                    continue
                if not tem_match(POSITIVE_TERMS, alvo):
                    continue
                itens.append({
                    "fonte": nome,
                    "titulo": texto[:160] or "(sem t√≠tulo)",
                    "resumo": "",
                    "link": href,
                    "publicado_em": "",
                    "metodo": "HTML",
                })
        except Exception as ex:
            print(f"[WARN] HTML falhou {nome}: {ex}")
    return itens

# --- Sa√≠das ---
def formatar_email(itens):
    if not itens:
        return f"Nenhuma promo√ß√£o relevante encontrada nos √∫ltimos {RECENCIA_DIAS} dias."
    def key_sort(x): return (x["publicado_em"] or "", x["fonte"], x["titulo"])
    linhas = [f"‚úàÔ∏è Promo√ß√µes (milhas + Accor/ALL) ‚Äì √∫ltimos {RECENCIA_DIAS} dias\n"]
    for i, it in enumerate(sorted(itens, key=key_sort, reverse=True), 1):
        dt_fmt = ""
        if it["publicado_em"]:
            try:
                dt_fmt = dtparser.parse(it["publicado_em"]).astimezone(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
            except Exception:
                dt_fmt = it["publicado_em"]
        linhas.append(f"{i}. [{it['fonte']}] {it['titulo']}\n   Data: {dt_fmt}\n   Link: {it['link']}\n")
    return "\n".join(linhas)

def enviar_email(corpo, assunto=None):
    caminho = "credenciais.txt"
    assert os.path.exists(caminho), f"Arquivo de credenciais n√£o encontrado: {caminho}"
    with open(caminho, "r", encoding="utf-8") as f:
        email_user, email_pass, email_to = [ln.strip() for ln in f if ln.strip()][:3]
    if not assunto:
        assunto = f"‚úàÔ∏è Promo√ß√µes (Milhas + Accor/ALL) ‚Äì √∫ltimos {RECENCIA_DIAS} dias"
    msg = MIMEMultipart(); msg["From"]=email_user; msg["To"]=email_to; msg["Subject"]=assunto
    msg.attach(MIMEText(corpo, "plain", "utf-8"))
    s = smtplib.SMTP("smtp.gmail.com", 587); s.starttls(); s.login(email_user, email_pass); s.send_message(msg); s.quit()

def salvar_csv(itens, caminho="monitor_promos_milhas_log.csv"):
    campos = ["timestamp_execucao_utc", "fonte", "titulo", "link", "publicado_em", "metodo"]
    ts = datetime.now(timezone.utc).isoformat()
    novo = not os.path.exists(caminho)
    with open(caminho, "a", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=campos)
        if novo: w.writeheader()
        for it in itens:
            w.writerow({
                "timestamp_execucao_utc": ts,
                "fonte": it["fonte"],
                "titulo": it["titulo"],
                "link": it["link"],
                "publicado_em": it["publicado_em"],
                "metodo": it["metodo"],
            })

def main():
    print(f"üîç Buscando promo√ß√µes (milhas + Accor/ALL), √∫ltimos {RECENCIA_DIAS} dias‚Ä¶")
    itens = coletar_rss() + coletar_html()
    corpo = formatar_email(itens)
    print("\n===== PR√âVIA DO EMAIL =====\n"); print(corpo); print("\n===========================\n")
    try:
        enviar_email(corpo); print("‚úÖ Email enviado com sucesso!")
    except Exception as ex:
        print("‚ùå Erro ao enviar e-mail:", ex, file=sys.stderr)
    try:
        salvar_csv(itens)
    except Exception as ex:
        print("[WARN] Falha ao salvar CSV:", ex, file=sys.stderr)

if __name__ == "__main__":
    main()
